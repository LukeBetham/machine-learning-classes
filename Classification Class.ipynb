{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T13:57:48.609068Z",
     "start_time": "2020-03-19T13:57:48.584969Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scikitplot as skplt\n",
    "\n",
    "# settings\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# import models from sklearn\n",
    "from sklearn import datasets, metrics, naive_bayes\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import scipy.stats as stats\n",
    "\n",
    "# skip warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# change cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T16:19:51.475382Z",
     "start_time": "2020-03-20T16:19:51.366450Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class full_classification:\n",
    "    \"\"\"A class which automatically does all classification models and gridsearches for you (logisitic default). \n",
    "    Note: when you run a new model it will overwrite the previous model. You can access the current model with .model and .model_des.\n",
    "    Other options:\n",
    "    run_all = default True, if set to false the class will not automatically run any models\n",
    "    baseline = default 0, set this to your baseline accuracy measure for comparision stats\n",
    "    standardize = default True, uses standard scaler and fit-transforms on train, transform on test if exists\n",
    "    test_size = default 0.15, decide the side of your test set - set to 0 if dont want test\n",
    "    folds = default 6, amount of folds for cross validation - integer > 1\n",
    "    shuffle = default True, shuffle the data for test split and cross val\n",
    "    stratify = default None, input the variable that you which to stratify the data by\n",
    "    print_info = default True, print all of the results out every time you run a model\n",
    "    save_it = default False, this adds functionality to be able to save down all model results into a\n",
    "              dataframe, set as a global variable called model_tracker.\n",
    "    comment = default None, This is a comment field for the model_tracker\n",
    "    Go to readme for further information: https://github.com/LukeBetham/machine-learning-classes/blob/master/README.md\n",
    "    Created by LukeBetham\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, run_all=True, baseline=0, standardize=True,\n",
    "                 test_size=0.15, folds=6, shuffle=True, stratify=None,\n",
    "                 print_info=True, save_it=False, comment=None):\n",
    "        # Save settings to object\n",
    "        self.folds = folds\n",
    "        self.shuffle = shuffle\n",
    "        self.stratify= stratify\n",
    "        self.comment = comment\n",
    "        self.save_it = save_it\n",
    "        self.print_info = print_info\n",
    "        if self.stratify is None:\n",
    "            self.kfold = KFold(self.folds, shuffle=self.shuffle, random_state=66)\n",
    "        else:\n",
    "            self.kfold = StratifiedKFold(self.folds, shuffle=self.shuffle, random_state=66)\n",
    "        # Option for bolding print text\n",
    "        self.BOLD = '\\033[1m'\n",
    "        self.END = '\\033[0m'\n",
    "        # Create train-test split if selected\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.baseline = baseline\n",
    "        self.test = test_size\n",
    "        if self.test != 0:\n",
    "            self.X, self.X_test, self.y, self.y_test, self.index_train, self.index_test = train_test_split(\n",
    "                self.X, self.y, self.X.index, test_size=self.test, shuffle=self.shuffle, \n",
    "                stratify=self.stratify, random_state=66)\n",
    "        # Standardise the data if selected\n",
    "        if standardize != 'none':\n",
    "            scaler = StandardScaler()\n",
    "            self.X = pd.DataFrame(\n",
    "                scaler.fit_transform(self.X), columns=self.X.columns)\n",
    "            if self.test != 0:\n",
    "                self.X_test = pd.DataFrame(\n",
    "                    scaler.transform(self.X_test), columns=self.X.columns)\n",
    "        # Run all models\n",
    "        if run_all==True:\n",
    "            self.knn_model()\n",
    "            self.decision_tree_model()\n",
    "            self.logistic_model()\n",
    "            self.random_forest_model()\n",
    "            self.ADAboosting_model()\n",
    "            self.GradientBoosting()\n",
    "            self.NaiveBayes()\n",
    "            self.LinearSVC()\n",
    "            #only enable this one if small dataset or want to wait a while\n",
    "            self.PolynomialSVC()\n",
    "            self.GaussianSVC()\n",
    "\n",
    "    def logistic_model(self, Logistic=LogisticRegression()):\n",
    "        # Set up Logistic Regresssion\n",
    "        self.model = Logistic\n",
    "        self.model_des = \"Logistic Regression Model\"\n",
    "        self.grid_multiple = 7\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Run .coefs() to see coef dataframe\")\n",
    "\n",
    "    def knn_model(self, KNN=KNeighborsClassifier(n_neighbors=10)):\n",
    "        # set up KNN model\n",
    "        self.model = KNN\n",
    "        self.model_des = \"K Neighbors Model\"\n",
    "        self.grid_multiple = 0.5\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Run .knn_all_k() to run all possible k values and find best k value.\")\n",
    "\n",
    "    def knn_all_k(self, limit = 50):\n",
    "        # run KNN for all possible Ks and graph them\n",
    "        self.scores = []\n",
    "        self.max_k = np.minimum(limit,int(len(self.y)*(1-(1/self.folds))-1))\n",
    "        for k in range(1, self.max_k):\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            self.scores.append(np.mean(cross_val_score(knn, self.X, self.y, cv=self.kfold)))\n",
    "        self.knn_best = self.scores.index(np.max(self.scores))+1\n",
    "        plt.plot(range(1, self.max_k), self.scores, label='Mean CV Scores')\n",
    "        plt.hlines(self.baseline, 1, self.max_k, label='baseline')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(loc=[1.1, 0])\n",
    "        print(self.BOLD + \"Highest KNN Score:\" + self.END, self.knn_best)\n",
    "        plt.show()\n",
    "\n",
    "    def decision_tree_model(self, print_tree=False, DecisionTree=DecisionTreeClassifier(random_state=66)):\n",
    "        # set up decision tree model\n",
    "        self.model = DecisionTree\n",
    "        self.model_des = \"Decision Tree Model\"\n",
    "        self.grid_multiple = 9\n",
    "        self.model_calc()\n",
    "        if print_tree == True:\n",
    "            dot_data = StringIO() \n",
    "            export_graphviz(self.model, out_file=dot_data, filled=True, rounded=True,\n",
    "                            special_characters=True, feature_names=self.X.columns)  \n",
    "\n",
    "            graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "            display(Image(graph.create_png()))\n",
    "\n",
    "    def random_forest_model(self, forest=RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, random_state=66)):\n",
    "        self.model = forest\n",
    "        self.model_des = \"Random Forest Model\"\n",
    "        self.grid_multiple = 15.25\n",
    "        self.model_calc()\n",
    "\n",
    "    def ADAboosting_model(self, plot_it=False, estimators=100, base_estimator=DecisionTreeClassifier(max_depth=3,random_state=66)):\n",
    "        self.model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=estimators,\n",
    "                                        algorithm='SAMME', random_state=66)\n",
    "        self.model_des = \"ADA Boosting Model\"\n",
    "        self.grid_multiple = 2.5\n",
    "        self.model_calc()\n",
    "        # plot\n",
    "        if plot_it == True:\n",
    "            plt.plot(list(self.model.staged_score(self.X, self.y)),\n",
    "                     label='training score', lw=2)\n",
    "            plt.plot(list(self.model.staged_score(\n",
    "                self.X_test, self.y_test)), label='test score', lw=2)\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('score')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "    def GradientBoosting(self, grad_model=GradientBoostingClassifier(n_estimators=100)):\n",
    "        self.model = grad_model\n",
    "        self.model_des = \"Gradient Boosting Model\"\n",
    "        self.grid_multiple = 32.4\n",
    "        self.model_calc()\n",
    "    \n",
    "    def NaiveBayes(self, nbtype = naive_bayes.GaussianNB(),power_transform=False):\n",
    "        self.model = nbtype\n",
    "        self.model_des = \"Naive Bayes Model\"\n",
    "        self.grid_multiple = 0.75\n",
    "        # With the option to power transform to make distribution more normal for the GaussianNB          \n",
    "        if power_transform == True:\n",
    "            X_temp, X_test_temp = self.X.copy(), self.X_test.copy()\n",
    "            power = PowerTransformer()\n",
    "            self.X = pd.DataFrame(power.fit_transform(self.X),columns=self.X_tp.columns)\n",
    "            self.X_test = power.transform(self.X_test)\n",
    "            self.model_des = self.model_des +\" with Power Transform\"\n",
    "            self.model_calc()\n",
    "            self.X, self.X_test = X_temp.copy(), X_test_temp.copy()\n",
    "        else:\n",
    "            self.model_calc()\n",
    "    \n",
    "    def LinearSVC(self,svc = LinearSVC(C=1, loss=\"hinge\")):\n",
    "        self.model = svc\n",
    "        self.model_des = \"Linear Support Vectors Model\"\n",
    "        self.grid_multiple = 0.3                  \n",
    "        self.model_calc()\n",
    "        \n",
    "    def PolynomialSVC(self,psvc = SVC(kernel=\"poly\", degree=3, coef0=1, C=5)):\n",
    "        self.model = psvc\n",
    "        self.model_des = \"Polynomial Support Vectors Model\"\n",
    "        self.grid_multiple = 2\n",
    "        self.model_calc()\n",
    "        \n",
    "    def GaussianSVC(self, gsvc = SVC(kernel=\"rbf\", gamma=5, C=0.001)):\n",
    "        self.model = gsvc\n",
    "        self.model_des = \"Gaussian (rbf) Support Vectors Model\"\n",
    "        self.grid_multiple = 1.3\n",
    "        self.model_calc()\n",
    "    \n",
    "    def MLP_Neural_Net(self,params = MLPClassifier(solver='adam', alpha=10**(0),\n",
    "                                        hidden_layer_sizes=(10, 10, 10), activation='relu',\n",
    "                                        random_state=66, batch_size=50,max_iter=500)):\n",
    "        self.model = params\n",
    "        self.model_des = 'MLP Classifier Neural Net'\n",
    "        self.grid_multiple = 38.5\n",
    "        self.model_calc()\n",
    "\n",
    "    def coefs(self):\n",
    "        self.dfc = pd.DataFrame(self.coef, columns=self.X.columns)\n",
    "        return self.dfc\n",
    "\n",
    "    def model_calc(self):\n",
    "        # fit model\n",
    "        t0 = time.time()\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.sc = self.model.score(self.X, self.y)\n",
    "        self.cvs = cross_val_score(self.model, self.X, self.y, cv=self.kfold).mean()\n",
    "        # Get test score\n",
    "        if self.test != 0:\n",
    "            self.sct = self.model.score(self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        else:\n",
    "            self.sctp = None\n",
    "        # time the running of the model\n",
    "        t1 = time.time()\n",
    "        self.elaspsed = t1-t0\n",
    "        # show the results from the classification model\n",
    "        if self.print_info==True:\n",
    "            print(\"\\n\",self.BOLD + self.model_des, 'Test\\nModel Score:' + self.END, round(self.sc, 4), \"- better than baseline by\", round(self.sc-self.baseline, 4),\n",
    "                  self.BOLD + '\\nCV Fold Score:' +\n",
    "                  self.END, round(\n",
    "                      self.cvs, 4), \"- better than baseline by\", round(self.cvs-self.baseline, 4),\n",
    "                  self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*self.grid_multiple, 2), 'minutes to run.')\n",
    "        try:\n",
    "            self.coef = self.model.coef_\n",
    "            self.coefs\n",
    "        except:\n",
    "            pass\n",
    "        if self.save_it == True:\n",
    "            self.tracking()\n",
    "            print(\"Saved model to global var  = model_tracker\")\n",
    "            \n",
    "    def tracking(self):\n",
    "        global model_tracker\n",
    "        df_temp = pd.DataFrame({'model_type':self.model_des,'model_train_score':self.sc,\n",
    "                                'cv_score':self.cvs,'test_score':self.sct,'predictors': str(','.join(self.X.columns)),\n",
    "                                'baseline':self.baseline,'cv_above_baseline':self.cvs-self.baseline,\n",
    "                                'model_params':str(self.model),'time':self.elaspsed,'comment':self.comment},index=[1])\n",
    "        try:\n",
    "            model_tracker =  pd.concat([model_tracker,df_temp])\n",
    "        except:\n",
    "            model_tracker = pd.DataFrame(columns = ['model_type','model_train_score','cv_score',\n",
    "                                                    'test_score','predictors','baseline',\n",
    "                                                    'cv_above_baseline','model_params','time',\n",
    "                                                    'comment'])\n",
    "            model_tracker =  pd.concat([model_tracker,df_temp])\n",
    "\n",
    "    def gridsearch(self, params='default'):\n",
    "        \"\"\"A function which automatically runs a gridsearch on your selected model. Returns model_grid model with best parameters.\n",
    "        Has default parameters for each model type, but you can set your own by passing a dict into params = {}\n",
    "        \"\"\"\n",
    "        # setting the default parameters if not set by user\n",
    "        if params == 'default':\n",
    "            if self.model_des == \"Logistic Regression Model\":\n",
    "                self.params = {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'C': np.logspace(-5, 5, 5), 'l1_ratio': np.linspace(0.0001, 1, 4)}\n",
    "            elif self.model_des == \"K Neighbors Model\":\n",
    "                self.params = {'n_neighbors': range(1, 20, 1), 'weights': ['uniform', 'distance'], 'p': [1, 2]}\n",
    "            elif self.model_des == \"Decision Tree Model\":\n",
    "                self.params = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 6, 7, 8], 'max_features': ['auto'], 'splitter': [\n",
    "                    'best', 'random'], 'min_samples_split': [2, 3, 4, 5], 'ccp_alpha': [0.0, 0.0001, 0.001, .01, .1, 1, 10, 100], 'class_weight': [None, 'balanced']}\n",
    "            elif self.model_des == \"Random Forest Model\":\n",
    "                self.params = {'n_estimators':[100,200,500], 'criterion':['gini','entropy'], 'max_depth':[None], 'min_samples_split':[2,6],\"max_features\":[\"auto\",\"log2\"],\n",
    "                               'oob_score':[True,False],'warm_start':[True,False],'ccp_alpha':[0.0,0.5,1]}\n",
    "            elif self.model_des == \"ADA Boosting Model\": \n",
    "                self.params = {\"learning_rate\": [0.05, 0.25, 0.5, 0.75, 1], 'base_estimator':[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4),DecisionTreeClassifier(max_depth=5)],\n",
    "                               'algorithm':['SAMME'],\"n_estimators\":[100,200,500,1000]}\n",
    "            elif self.model_des == \"Gradient Boosting Model\": \n",
    "                self.params = {\"learning_rate\": [0.01, 0.5, 1], 'loss':['deviance', 'exponential'],'max_features':['auto','log2','sqrt'],\n",
    "                               'warm_start':[True,False],\"n_estimators\":[100,200], 'ccp_alpha':[0.0,0.5,0.9],'max_depth':[1,3,5], 'subsample':[1.0,0.75,0.5]}\n",
    "            elif self.model_des == \"Naive Bayes Model\":\n",
    "                self.params = {\"var_smoothing\": [0.000001, 0.2, 0.4, 0.6, 0.8, 1]}\n",
    "            elif self.model_des == \"Linear Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,20),'loss':['hinge','squared_hinge']}\n",
    "            elif self.model_des == \"Gaussian (rbf) Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,15),'gamma':np.linspace(0.00001,100,15),'kernel':['rbf']}\n",
    "            elif self.model_des == \"Polynomial Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,15),'coef0':[0,1,2,3,4,10],'kernel':['poly'],'degree':[2,3,4]}\n",
    "            elif self.model_des == \"MLP Classifier Neural Net\": \n",
    "                self.params = {'solver':['adam','sgd'], 'alpha': np.linspace(0.00001,1,4),'hidden_layer_sizes':[(10, 10, 10,10),(20, 20, 20),(50,50),(100)],\n",
    "                               'learning_rate' : ['constant', 'invscaling', 'adaptive'],'activation' : ['identity', 'logistic', 'tanh', 'relu']}\n",
    "        else:\n",
    "            self.params = params\n",
    "        # setup the gridsearch\n",
    "        self.grid = GridSearchCV(self.model, self.params, verbose=1, cv=StratifiedKFold(\n",
    "            self.folds, shuffle=self.shuffle, random_state=66))\n",
    "        self.grid.fit(self.X, self.y)\n",
    "        self.gsc = self.grid.best_score_\n",
    "        self.best = self.grid.best_params_\n",
    "        self.model = self.grid.best_estimator_\n",
    "        self.model_des = self.model_des + \" Grid Search:\"\n",
    "        try:\n",
    "            self.coef = self.grid.best_estimator_.coef_\n",
    "        except:\n",
    "            pass\n",
    "        # Check test score for grid\n",
    "        try:\n",
    "            self.sct = self.grid.best_estimator_.score(\n",
    "                self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        except:\n",
    "            self.sctp = None\n",
    "        # Print Grid results\n",
    "        if self.print_info==True:\n",
    "            print(self.BOLD + self.model_des + self.END)\n",
    "            print(self.BOLD + \"Best Mean CV Model Score:\" + self.END, round(self.gsc, 4), \"- which is better than baseline by\",\n",
    "                  round(self.gsc-self.baseline, 4), self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "            print(self.BOLD + 'Grid Best Parameters:\\n' + self.END, self.best)\n",
    "            print(self.BOLD + '\\nSearch Parameters:\\n' + self.END, self.params)\n",
    "        self.coefs()\n",
    "\n",
    "    def matrix_n_graphs(self, normalize=True):\n",
    "        print(self.BOLD + self.model_des, \"on X_test\" + self.END)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        skplt.metrics.plot_confusion_matrix(\n",
    "            self.y_test, self.y_pred, figsize=(8, 8), labels=[0, 1], normalize=normalize)\n",
    "        plt.xlim(-0.5, len(self.model.classes_)-0.5)\n",
    "        plt.ylim(len(self.model.classes_)-0.5, -0.5)\n",
    "        plt.show()\n",
    "        cmap = ListedColormap(sns.color_palette(\"husl\", 3))\n",
    "        skplt.metrics.plot_roc(self.y_test, self.model.predict_proba(self.X_test), plot_micro=False,\n",
    "                               plot_macro=False, title_fontsize=20, text_fontsize=16, figsize=(8, 8), cmap=cmap)\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        skplt.metrics.plot_precision_recall(self.y_test, self.model.predict_proba(\n",
    "            self.X_test), plot_micro=False, title_fontsize=20, text_fontsize=16, cmap=cmap, ax=ax)\n",
    "        ax.legend(loc=[1.1, 0])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
