{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:33:00.413819Z",
     "start_time": "2020-01-21T18:33:00.318200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scikitplot as skplt\n",
    "\n",
    "# settings\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# import models from sklearn\n",
    "from sklearn import datasets, metrics, naive_bayes\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import scipy.stats as stats\n",
    "\n",
    "# skip warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:23:16.447633Z",
     "start_time": "2020-01-21T17:23:16.398971Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class full_classification:\n",
    "    \"\"\"A class which automatically does all classification models and gridsearches for you (logisitic default). \n",
    "    Remember to input baseline figure and decide if you want standardisation.\n",
    "    Note: when you run a new model it will overwrite the previous model. You can access the current model with .model and .model_des.\n",
    "    Created by LukeBetham\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, run_all=True, baseline=0, standardize=True, test_size=0.15, folds=6, shuffle=True, stratify=None,print_info=True, save_it=False, comment=None):\n",
    "\n",
    "        # Save settings to object\n",
    "        self.folds = folds\n",
    "        self.shuffle = shuffle\n",
    "        self.stratify= stratify\n",
    "        # Option for bolding print text\n",
    "        self.BOLD = '\\033[1m'\n",
    "        self.END = '\\033[0m'\n",
    "        # Create train-test if selected\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.baseline = baseline\n",
    "        self.test = test_size\n",
    "        if self.test != 0:\n",
    "#             self.X, self.X_test, self.y, self.y_test, self.index_train, self.index_test = train_test_split(\n",
    "#                 self.X, self.y, self.X.index, test_size=self.test, shuffle=self.shuffle, stratify=self.stratify, random_state=66)\n",
    "            self.X, self.X_test, self.y, self.y_test, self.index_train, self.index_test, self.Xf_train, self.Xf_test, self.yf_train, self.yf_test = train_test_split(\n",
    "                self.X, self.y, self.X.index, X_flip, y_flip, test_size=self.test, shuffle=self.shuffle, stratify=self.stratify, random_state=66)\n",
    "#         # Standardise the data if selected\n",
    "        if standardize != 'none':\n",
    "            scaler = StandardScaler()\n",
    "            self.X = pd.DataFrame(\n",
    "                scaler.fit_transform(self.X), columns=self.X.columns)\n",
    "            if self.test != 0:\n",
    "                self.X_test = pd.DataFrame(\n",
    "                    scaler.transform(self.X_test), columns=self.X.columns)\n",
    "#             Delete this bit when done\n",
    "            self.Xf_train = pd.DataFrame(\n",
    "                    scaler.fit_transform(self.Xf_train), columns=self.X.columns)\n",
    "            self.Xf_test = pd.DataFrame(\n",
    "                    scaler.transform(self.Xf_test), columns=self.X.columns)\n",
    "        #Settings\n",
    "        self.comment = comment\n",
    "        self.save_it = save_it\n",
    "        self.print_info = print_info\n",
    "        # Call the 5 standard models\n",
    "        if run_all==True:\n",
    "            self.knn_model(5)\n",
    "            self.decision_tree_model()\n",
    "            self.logistic_model()\n",
    "            self.random_forest_model()\n",
    "            self.ADAboosting_model()\n",
    "            self.GradientBoosting()\n",
    "            self.NaiveBayes()\n",
    "            self.LinearSVC()\n",
    "            #only enable this one if small dataset or want to wait a while\n",
    "            #self.PolynomialSVC()\n",
    "            #self.GaussianSVC()\n",
    "\n",
    "\n",
    "    def logistic_model(self, Logistic=LogisticRegression(fit_intercept=False)):\n",
    "        # Set up Logistic Regresssion\n",
    "        self.model = Logistic\n",
    "        self.model_des = \"Logistic Regression Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Note: default changed to remove y-intercept for this model.\")\n",
    "            print(\"Run .coefs() to see coef dataframe\\nTime Elapsed = \", round(self.elaspsed, 2),\n",
    "                  'secs - grid will take ~', round(self.elaspsed*30, 2), 'minutes to run.\\n')\n",
    "\n",
    "    def knn_model(self, k='all', weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None):\n",
    "        if k != 'all':\n",
    "            # set up KNN model\n",
    "            self.model = KNeighborsClassifier(n_neighbors=k, weights=weights, algorithm=algorithm,\n",
    "                                              leaf_size=leaf_size, p=p, metric=metric, metric_params=metric_params, n_jobs=n_jobs)\n",
    "            self.model_des = \"K Neighbors Model\"\n",
    "            self.model_calc()\n",
    "            if self.print_info==True:\n",
    "                print(\"Set k='all' to run full set of ks and graph.\\nTime Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~', round(\n",
    "                    self.elaspsed*7, 2), 'minutes to run - and all ks', round(int(len(self.y)*(1-(1/self.folds))-1)*self.elaspsed/120, 2), 'mins\\n')\n",
    "\n",
    "        else:\n",
    "            # run KNN for all possible Ks and graph them\n",
    "            self.scores = []\n",
    "            self.max_k = int(len(self.y)*(1-(1/self.folds))-1)\n",
    "            for k in range(1, self.max_k):\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights=weights, algorithm=algorithm,\n",
    "                                           leaf_size=leaf_size, p=p, metric=metric, metric_params=metric_params, n_jobs=n_jobs)\n",
    "                self.scores.append(np.mean(cross_val_score(knn, self.X, self.y, cv=StratifiedKFold(\n",
    "                    self.folds, shuffle=self.shuffle, random_state=66))))\n",
    "            self.knn_best = self.scores.index(np.max(self.scores))+1\n",
    "            plt.plot(range(1, self.max_k), self.scores, label='Mean CV Scores')\n",
    "            plt.hlines(self.baseline, 1, self.max_k, label='baseline')\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.legend(loc=[1.1, 0])\n",
    "            print(self.BOLD + \"Highest KNN Score:\" + self.END, self.knn_best)\n",
    "            plt.show()\n",
    "\n",
    "    def decision_tree_model(self, print_tree=False, DecisionTree=DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, random_state=66)):\n",
    "        # set up decision tree model\n",
    "        self.model = DecisionTree\n",
    "        self.model_des = \"Decision Tree Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*50, 2), 'minutes to run.\\n')\n",
    "        if print_tree == True:\n",
    "            dot_data = StringIO() \n",
    "            export_graphviz(self.model, out_file=dot_data, filled=True, rounded=True,\n",
    "                            special_characters=True, feature_names=self.X.columns)  \n",
    "\n",
    "            graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "            display(Image(graph.create_png()))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def random_forest_model(self, forest=RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, random_state=66)):\n",
    "        self.model = forest\n",
    "        self.model_des = \"Random Forest Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*9.5, 2), 'minutes to run.\\n')\n",
    "\n",
    "    def ADAboosting_model(self, plot_it=False, estimators=100, base_estimator=DecisionTreeClassifier(max_depth=3,random_state=66)):\n",
    "        self.model = AdaBoostClassifier(\n",
    "            base_estimator=base_estimator, n_estimators=estimators, algorithm='SAMME', random_state=66)\n",
    "        self.model_des = \"ADA Boosting Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*2.5, 2), 'minutes to run.\\n')\n",
    "        # plot\n",
    "        if plot_it == True:\n",
    "            plt.plot(list(self.model.staged_score(self.X, self.y)),\n",
    "                     label='training score', lw=2)\n",
    "            plt.plot(list(self.model.staged_score(\n",
    "                self.X_test, self.y_test)), label='test score', lw=2)\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('score')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "    def GradientBoosting(self, grad_model=GradientBoostingClassifier(n_estimators=100)):\n",
    "        self.model = grad_model\n",
    "        self.model_des = \"Gradient Boosting Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*2.5, 2), 'minutes to run.\\n')\n",
    "    \n",
    "    def NaiveBayes(self, nbtype = naive_bayes.GaussianNB(),power_transform=False):\n",
    "        self.model = nbtype\n",
    "        self.model_des = \"Naive Bayes Model\"\n",
    "        if power_transform == True:\n",
    "            self.X_tp, self.X_test_tp = self.X.copy(),self.X_test.copy()\n",
    "            power = PowerTransformer()\n",
    "            self.X = pd.DataFrame(power.fit_transform(self.X),columns=self.X_tp.columns)\n",
    "            self.X_test = power.transform(self.X_test)\n",
    "            self.model_des = self.model_des +\" with Power Transform\"\n",
    "            self.model_calc()\n",
    "            self.X, self.X_test = self.X_tp.copy(), self.X_test_tp.copy()\n",
    "        else:\n",
    "            self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*2.5, 2), 'minutes to run.\\n')\n",
    "    \n",
    "    def LinearSVC(self,svc = LinearSVC(C=1, loss=\"hinge\")):\n",
    "        self.model = svc\n",
    "        self.model_des = \"Linear Support Vectors Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*0.5, 2), 'minutes to run.\\n')\n",
    "        \n",
    "    def PolynomialSVC(self,psvc = SVC(kernel=\"poly\", degree=3, coef0=1, C=5)):\n",
    "        self.model = psvc\n",
    "        self.model_des = \"Polynomial Support Vectors Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*4.5, 2), 'minutes to run.\\n')\n",
    "        \n",
    "    def GaussianSVC(self,gsvc = SVC(kernel=\"rbf\", gamma=5, C=0.001)):\n",
    "        self.model = gsvc\n",
    "        self.model_des = \"Gaussian (rbf) Support Vectors Model\"\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*15, 2), 'minutes to run.\\n')\n",
    "    \n",
    "    def MLP_Neural_Net(self,params = MLPClassifier(solver='adam', alpha=10**(0),\n",
    "                                        hidden_layer_sizes=(10, 10, 10), activation='relu',\n",
    "                                        random_state=42, batch_size=50,max_iter=500)):\n",
    "        self.model = params\n",
    "        self.model_des = 'MLP Classifier Neural Net'\n",
    "        self.model_calc()\n",
    "        if self.print_info==True:\n",
    "            print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "                  round(self.elaspsed*15, 2), 'minutes to run.\\n')\n",
    "\n",
    "    def coefs(self):\n",
    "        self.dfc = pd.DataFrame(self.coef, columns=self.X.columns)\n",
    "        return self.dfc\n",
    "\n",
    "    def model_calc(self):\n",
    "        # fit model\n",
    "        t0 = time.time()\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.sc = self.model.score(self.X, self.y)\n",
    "        #add in some code to switch between stratified Kfold and non\n",
    "        self.cvs = cross_val_score(self.model, self.X, self.y, cv=StratifiedKFold(\n",
    "            self.folds, shuffle=self.shuffle, random_state=66)).mean()\n",
    "        # Get test score\n",
    "        if self.test != 0:\n",
    "            self.sct = self.model.score(self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        else:\n",
    "            self.sctp = None\n",
    "        t1 = time.time()\n",
    "        self.elaspsed = t1-t0\n",
    "        # show the results from the classification model\n",
    "        if self.print_info==True:\n",
    "            print(self.BOLD + self.model_des, 'Test\\nModel Score:' + self.END, round(self.sc, 4), \"- better than baseline by\", round(self.sc-self.baseline, 4),\n",
    "                  self.BOLD + '\\nCV Fold Score:' +\n",
    "                  self.END, round(\n",
    "                      self.cvs, 4), \"- better than baseline by\", round(self.cvs-self.baseline, 4),\n",
    "                  self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "        try:\n",
    "            self.coef = self.model.coef_\n",
    "            self.coefs\n",
    "        except:\n",
    "            pass\n",
    "#         print(\"Use .gridsearch() to run full regularisation tests using all default for current model.\",\n",
    "#               \"\\nUse .knn_model() or .logistic_model() or .decision_tree_model() to change model and specify paramters.\")\n",
    "        if self.save_it == True:\n",
    "            self.tracking()\n",
    "            print(\"Saved model to global model tracker.\")\n",
    "#         DELETE WHEN DONE\n",
    "        if self.print_info == True:\n",
    "            print(\"Flipped_Train = \",round(self.model.score(self.Xf_train, self.yf_train),4))\n",
    "            print(\"Flipped_Test = \",round(self.model.score(self.Xf_test, self.yf_test),4))\n",
    "        \n",
    "    def tracking(self):\n",
    "        global model_tracker\n",
    "        df_temp = pd.DataFrame({'model_type':self.model_des,'model_train_score':self.sc,\n",
    "                                'cv_score':self.cvs,'test_score':self.sct,'predictors': str(','.join(self.X.columns)),\n",
    "                                'baseline':self.baseline,'cv_above_baseline':self.cvs-self.baseline,\n",
    "                                'model_params':str(self.model),'time':self.elaspsed,'comment':self.comment},\n",
    "                               index=[len(model_tracker.index)])\n",
    "        model_tracker =  pd.concat([model_tracker,df_temp])\n",
    "\n",
    "    def gridsearch(self, params='default'):\n",
    "        \"\"\"A function which automatically runs a gridsearch on your selected model. Returns model_grid model with best parameters.\n",
    "        Defaults for Logistic (600 iterations): {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'C': np.logspace(-5, 5, 5), 'l1_ratio': np.linspace(0.0001, 1, 4)}\n",
    "        Defaults for KNN: self.params (100 iterations) = {'n_neighbors':range(1,20,1), 'weights':['uniform','distance'], 'p':[1,2]}         \n",
    "        Defaults for Decision Tree (1000 iterations) = {'criterion':['gini','entropy'],'max_depth': [None,5,6,7,8],'max_features':['auto'],'splitter':['best','random'],'min_samples_split':[2,3,4,5],'ccp_alpha':[0.0,0.0001,0.001,.01,.1,1,10,100],'class_weight':[None,'balanced']}        \n",
    "        Defaults for Random Forest (575 iterations) = {'n_estimators':[100,200,500], 'criterion':['gini':'entropy'], 'max_depth':[None], 'min_samples_split':[2,4,6],\"max_features\":[\"auto\",\"log2\"],'oob_score':[True,False],'warm_start':[True,False],'ccp_alpha'=[0.0,0.5,1,10]}      \n",
    "        Defaults for ADABoosting Model  (150 iterations)  = {\"learning_rate\": [0.05, 0.25, 0.5, 0.75, 1], \"max_depth\":[1,2,3,4,5],\"max_features\":[\"auto\",\"log2\"],\"n_estimators\":[100,200,500]}      \n",
    "                \"\"\"\n",
    "        # setting the default parameters if not set by user\n",
    "        if params == 'default':\n",
    "            if self.model_des == \"Logistic Regression Model\":\n",
    "                self.params = {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'C': np.logspace(-5, 5, 5), 'l1_ratio': np.linspace(0.0001, 1, 4)}\n",
    "            elif self.model_des == \"K Neighbors Model\":\n",
    "                self.params = {'n_neighbors': range(1, 20, 1), 'weights': ['uniform', 'distance'], 'p': [1, 2]}\n",
    "            elif self.model_des == \"Decision Tree Model\":\n",
    "                self.params = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 6, 7, 8], 'max_features': ['auto'], 'splitter': [\n",
    "                    'best', 'random'], 'min_samples_split': [2, 3, 4, 5], 'ccp_alpha': [0.0, 0.0001, 0.001, .01, .1, 1, 10, 100], 'class_weight': [None, 'balanced']}\n",
    "            elif self.model_des == \"Random Forest Model\":\n",
    "                self.params = {'n_estimators':[100,200,500], 'criterion':['gini','entropy'], 'max_depth':[None], 'min_samples_split':[2,6],\"max_features\":[\"auto\",\"log2\"],\n",
    "                               'oob_score':[True,False],'warm_start':[True,False],'ccp_alpha':[0.0,0.5,1]}\n",
    "            elif self.model_des == \"ADA Boosting Model\": \n",
    "                self.params = {\"learning_rate\": [0.05, 0.25, 0.5, 0.75, 1], 'base_estimator':[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4),DecisionTreeClassifier(max_depth=5)],\n",
    "                               'algorithm':['SAMME'],\"n_estimators\":[100,200,500,1000]}\n",
    "            elif self.model_des == \"Gradient Boosting Model\":  \n",
    "                pass\n",
    "            elif self.model_des == \"Naive Bayes Model\":\n",
    "                pass\n",
    "            elif self.model_des == \"Linear Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,20),'loss':['hinge','squared_hinge']}\n",
    "            elif self.model_des == \"Gaussian (rbf) Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,15),'gamma':np.linspace(0.00001,100,15),'kernel':['rbf']}\n",
    "            elif self.model_des == \"Polynomial Support Vectors Model\": \n",
    "                self.params = {'C':np.linspace(-10,10,15),'coef0':[0,1,2,3,4,10],'kernel':['poly'],'degree':[2,3,4]}\n",
    "        else:\n",
    "            self.params = params\n",
    "\n",
    "        # setup the gridsearch\n",
    "        self.grid = GridSearchCV(self.model, self.params, verbose=1, cv=StratifiedKFold(\n",
    "            self.folds, shuffle=self.shuffle, random_state=66))\n",
    "        self.grid.fit(self.X, self.y)\n",
    "        self.gsc = self.grid.best_score_\n",
    "        self.best = self.grid.best_params_\n",
    "        self.model = self.grid.best_estimator_\n",
    "        self.model_des = self.model_des + \" Grid Search:\"\n",
    "        try:\n",
    "            self.coef = self.grid.best_estimator_.coef_\n",
    "        except:\n",
    "            pass\n",
    "        # Check test score for grid\n",
    "        try:\n",
    "            self.sct = self.grid.best_estimator_.score(\n",
    "                self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        except:\n",
    "            self.sctp = None\n",
    "        # Print Grid results\n",
    "        if self.print_info==True:\n",
    "            print(self.BOLD + self.model_des + self.END)\n",
    "            print(self.BOLD + \"Best Mean CV Model Score:\" + self.END, round(self.gsc, 4), \"- which is better than baseline by\",\n",
    "                  round(self.gsc-self.baseline, 4), self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "            print(self.BOLD + 'Grid Best Parameters:\\n' + self.END, self.best)\n",
    "            print(self.BOLD + '\\nSearch Parameters:\\n' + self.END, self.params)\n",
    "        self.coefs()\n",
    "\n",
    "    def matrix_n_graphs(self, normalize=True):\n",
    "        print(self.BOLD + self.model_des, \"on X_test\" + self.END)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        skplt.metrics.plot_confusion_matrix(\n",
    "            self.y_test, self.y_pred, figsize=(8, 8), labels=[0, 1], normalize=normalize)\n",
    "        plt.xlim(-0.5, len(self.model.classes_)-0.5)\n",
    "        plt.ylim(len(self.model.classes_)-0.5, -0.5)\n",
    "        plt.show()\n",
    "        cmap = ListedColormap(sns.color_palette(\"husl\", 3))\n",
    "        skplt.metrics.plot_roc(self.y_test, self.model.predict_proba(self.X_test), plot_micro=False,\n",
    "                               plot_macro=False, title_fontsize=20, text_fontsize=16, figsize=(8, 8), cmap=cmap)\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        skplt.metrics.plot_precision_recall(self.y_test, self.model.predict_proba(\n",
    "            self.X_test), plot_micro=False, title_fontsize=20, text_fontsize=16, cmap=cmap, ax=ax)\n",
    "        ax.legend(loc=[1.1, 0])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
